{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class + related functions to work with entire Micrometeoroid Impact Catalog\n",
    "\n",
    "Written by Sophie Hourihane - based on work by Ira Thorpe and Nicole Pagane\n",
    "\n",
    "\n",
    "8/1/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import pathlib\n",
    "import healpy as hp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from impactClass import impactClass\n",
    "from impactClass import impactClassList\n",
    "import copy\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def getGRSSegments(filenames, grs = 1):\n",
    "    # Get list of segment times\n",
    "    regex = r'(\\d*)_grs%i.*'%(grs)\n",
    "    segments = []\n",
    "    for f in filenames:\n",
    "        if re.match(regex, f):\n",
    "            segments.append(int(re.findall(regex, f)[0]))\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment\n",
    "Assuming your directory structure is correct & you have the pickle files, everything should work out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup directory structure\n",
    "p = pathlib.PurePath(os.getcwd())\n",
    "BASE_DIR = str(p.parent)\n",
    "dataDir = '/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Impact Class List\n",
    "This is a class of impacts and related functions. It seemed like a good idea but I really just needed a sorting function for a list\n",
    "- `grs`: int (1 or 2) Which GRS we are looking at\n",
    "    - default: 1\n",
    "- `getValid`: Bool, Only load impacts that have not been vetoed\n",
    "    - default: True\n",
    "- `BASE_DIR`: the base directory where /Analysis lives\n",
    "    - default: None, finds parent of cwd\n",
    "- `dataDir`: directory inside of /Analysis where all data lives\n",
    "- `directory`: the directory inside /Analysis where the pickle files we are trying to load live\n",
    "    - default: /data/ONLY_IMPACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading through pickle files\n"
     ]
    }
   ],
   "source": [
    "# IRA\n",
    "# I think the only change to be made is the directory, \n",
    "# for you it should be directory = '/data'\n",
    "# This takes a little bit to load, ~ 1 minute, \n",
    "# do NOT try to load every segment, just the impacts\n",
    "impactList = impactClassList(grs = 1, getValid = True, BASE_DIR = BASE_DIR, \n",
    "                             dataDir = dataDir, include_marginal = True,\n",
    "                             directory = dataDir + '/ONLY_IMPACTS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load ephemeris file\n",
    "fe = h5py.File(BASE_DIR + dataDir +'/lpfEphem.h5', 'r')\n",
    "dataset = fe['EME2000']\n",
    "dat = dataset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(impactList.impact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-04-09 & 1144229908 & $17.2^{+0.4}_{-0.3}$ & +y+y & 1729 & -7 & -7 & -57 & -39 & 1.09 & 0.55 & -0.05 \\\\\\\\'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impactList.impact_list[0].summaryString(ephem=dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Population model data\n",
    "Populations models made by Petr\n",
    "- `modelDir`: full path to population models\n",
    "- `pop_type`: string, one of ['JFC', 'HTC', 'AST', 'OCC', 'Uniform']\n",
    "- `usePtot`: Bool, whether to interpolate with momentum as well, should always be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from populationClass import population as pop\n",
    "modelDir = BASE_DIR + dataDir + '/models'\n",
    "usePtot = True  # Old boolean for ignoring momentum\n",
    "pop_names = ['JFC', 'HTC', 'AST', 'OCC', 'Uniform']\n",
    "populations = []\n",
    "# Takes about 30 seconds to read all populations in \n",
    "for p in pop_names:\n",
    "    print(p)\n",
    "    populations.append(pop(modelDir = modelDir, pop_type = p, usePtot = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSkyMap(self, percent_sky = 0.1, scale = 'log'):\n",
    "    from astropy.coordinates import SkyCoord\n",
    "    from astropy.io import fits\n",
    "    from astropy import units as u\n",
    "    import ligo.skymap.plot\n",
    "    import matplotlib as mpl\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "    ax = plt.axes(\n",
    "            projection = 'geo degrees mollweide')\n",
    "\n",
    "    # Copy shape of healPix\n",
    "    healPix_sun = self.impact_list[0].healPix_sun * 0\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    minimum = 1\n",
    "    maximum = 0\n",
    "    for i, impact in enumerate(self.impact_list):\n",
    "        if impact.skyArea_sun > percent_sky * 41253:\n",
    "            continue\n",
    "            \n",
    "        minimum = min(minimum, min(impact.healPix_sun[impact.healPix_sun > 0]))\n",
    "        maximum = max(maximum, max(impact.healPix_sun))\n",
    "\n",
    "    norm = mpl.colors.LogNorm(vmin = minimum, vmax = maximum)\n",
    "    for i, impact in enumerate(self.impact_list):\n",
    "        \n",
    "        if impact.skyArea_sun > percent_sky * 41253:\n",
    "            continue\n",
    "        count += 1\n",
    "        healPix_sun += impact.healPix_sun\n",
    "        #cax = ax.imshow_hpx(impact.healPix_sun, cmap = 'rainbow', norm = norm, alpha = 1) \n",
    "\n",
    "    \n",
    "    if 'log' in scale:\n",
    "        minimum = min(healPix_sun[healPix_sun > 0])\n",
    "        norm = mpl.colors.LogNorm(vmin = minimum, vmax = max(healPix_sun))\n",
    "    else:\n",
    "        norm = mpl.colors.Normalize(vmin = min(healPix_sun), vmax = max(healPix_sun))\n",
    "\n",
    "    cax = ax.imshow_hpx(healPix_sun, cmap = 'viridis', norm = norm) \n",
    "    fig.colorbar(cax)\n",
    "    return fig\n",
    "\n",
    "plotSkyMap(impactList, percent_sky = 0.1, scale = 'log')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Power Law\n",
    "Plots and fits a power law given a parameter like 'Ptot', 'lat', 'lon' etc\n",
    "- `param` : string, which parameter goes on the x axis \n",
    "    - default: 'Ptot'\n",
    "- `credibles`: List of floats < 1, What credible intervals to show\n",
    "    - default: [0.9]\n",
    "- `weight`: Bool, whether to weight fit on errors\n",
    "    - default: True\n",
    "- `drop_max`: Bool, whether to drop the maximum impact in the fit\n",
    "    - default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotPowerLaw(self, param = 'Ptot', credibles = [0.9], weight = True,\n",
    "                     drop_max = True):\n",
    "        \"\"\"\n",
    "        self = impactClassList instance\n",
    "        param = parameter plotted on x we fit for\n",
    "        credibles = credible intervals we want on the plot (list type)\n",
    "        weight = boolean, whether we want the plot weighted\n",
    "        drop_max = Boolean, wheteher to drop the max momentum in our impact list\n",
    "        \"\"\"\n",
    "        error_color = plt.cm.PuBu(np.linspace(.5, 1, len(credibles)))\n",
    "        fit_color = \"#e86e66\"\n",
    "        data_color = '#ffd27a'#'#92cac6'#\"#6b9bb9\"\n",
    "        import matplotlib as mpl\n",
    "        mpl.rcParams['lines.linewidth'] = 4\n",
    "        mpl.rcParams['font.size'] = 15\n",
    "        mpl.rcParams['font.family'] = 'sans-serif'\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "        # Sort data\n",
    "        sortlist = sorted(self.impact_list, key=lambda x: x.getMedian(param))\n",
    "        # Drops the largest number\n",
    "        shift = 0\n",
    "        if drop_max:\n",
    "            imp_max = sortlist[-1]\n",
    "            sortlist = sortlist[:-1]\n",
    "            shift = 1\n",
    "            # Replot the highest point\n",
    "            ax.scatter(imp_max.getMedian('Ptot'), 1, color = data_color)\n",
    "\n",
    "        xdata = np.asarray([i.getMedian(param) for i in sortlist]) #makeArray(sortlist, param, np.median))\n",
    "        ydata = np.arange(1 + shift, len(sortlist) + 1 + shift, 1.0)[::-1]\n",
    "        ydata = ydata/len(sortlist)\n",
    "\n",
    "\n",
    "        # Plot the data\n",
    "        ax.scatter(xdata, ydata, color = data_color, zorder = 10,\n",
    "                   label = 'Median Momentum', marker = 'o', s = 50)\n",
    "        lines = []\n",
    "        \n",
    "        \n",
    "        for i, credible in enumerate(credibles):\n",
    "            # Get Credible Intervals and stdev\n",
    "            cred_up, cred_down, stdev, median = self.getCredibleIntervals(sortlist, param,\n",
    "                                                                          credible, getMedian = True)\n",
    "\n",
    "            #Get power Law fit\n",
    "            if weight:\n",
    "                popt, pcov = self.fitPowerlaw(sortlist, param, sigma = 1 / (cred_up - cred_down))\n",
    "            else:\n",
    "                popt, pcov = self.fitPowerlaw(sortlist, param, sigma = None)\n",
    "\n",
    "            print('Optimized: a = ', popt[0], 'b =', popt[1] )\n",
    "\n",
    "            # Sets up points to plot for fit\n",
    "            plot_x = np.logspace(np.log10(min(xdata)), np.log10(max(xdata)), 1000)\n",
    "\n",
    "            if credible == .90:\n",
    "                ax.plot(plot_x, self.power_func(plot_x, *popt),\n",
    "                        color = fit_color,\n",
    "                        label = r'Data Fit: %1.2f$(\\frac{P}{[\\mu N s]})^{%1.2f}$'%(popt[0], popt[1]))\n",
    "\n",
    "            # Credible Intervals\n",
    "            ax.errorbar(xdata, ydata, xerr = [median - cred_down, cred_up - median],\n",
    "                 fmt = 'o',\n",
    "                        color = 'none',\n",
    "                        ecolor = error_color[i], label = '%i%% Credible'%(credible * 100),\n",
    "                 alpha = .75)\n",
    "            \n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel('Cumulative Number of Impacts')\n",
    "\n",
    "        if param == 'Ptot':\n",
    "            ax.set_xlabel('$p_{tot}\\,[\\mu N s]$')\n",
    "        else:\n",
    "            ax.set_xlabel(param)\n",
    "\n",
    "\n",
    "        ax.legend()\n",
    "        return fig, ax\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Without dropping the maxiumum impact\n",
    "fig, ax = plotPowerLaw(impactList, param = 'Ptot', credibles = [0.99, 0.9, 0.5], weight = True, \n",
    "                                  drop_max = False)\n",
    "ax.set_title('Maximum Impact Kept')\n",
    "\n",
    "\n",
    "# With dropping the maximum impact\n",
    "fig, ax = impactList.plotPowerLaw(param = 'Ptot', credibles = [0.99, 0.9, 0.5], weight = True, \n",
    "                                   drop_max = True)\n",
    "ax.set_title('Maximum Impact Dropped')\n",
    "\n",
    "# We can add the sum of the models line but we need to load the correct data\n",
    "def lin_func(x, m, b):\n",
    "    return m * x + b\n",
    "def power_func(x, a, b):\n",
    "    return a * (x ** b)\n",
    "\n",
    "sum_flux = np.zeros(9)\n",
    "for p in populations:\n",
    "    # We don't want to include uniform because its made up\n",
    "    if p.pop_type not in ['JFC', 'HTC', 'AST', 'OCC']:\n",
    "        continue\n",
    "        \n",
    "    # Groupby Momentum\n",
    "    grouped = p.df.groupby(['Ptot'])['flux'].sum().reset_index()\n",
    "    # Make Cumulative\n",
    "    for i in range(len(grouped.index)):\n",
    "        grouped['flux'][i] = np.sum(grouped['flux'][i:])\n",
    "        sum_flux[i] += np.sum(grouped['flux'][i:])\n",
    "\n",
    "# Surface rea of the LPF\n",
    "Area_SC = 10.377479175382778\n",
    "# Cross sectional area of the LPF\n",
    "cross_section_SC = 2\n",
    "total_time = 168 #days\n",
    "    \n",
    "# Fits the model\n",
    "time = total_time * 24 * 60 * 60 # seconds\n",
    "\n",
    "# Make momentum array\n",
    "xdata = np.asarray(grouped['Ptot'].values)\n",
    "ydata = np.asarray(sum_flux * time * cross_section_SC)\n",
    "popt, pcov = curve_fit(lin_func, np.log(xdata), np.log(ydata), p0 = (1, 1))\n",
    "b = popt[0]\n",
    "a = np.exp(popt[1])\n",
    "popt = [a, b]\n",
    "# Sets up points to plot for fit\n",
    "plot_x = np.logspace(np.log10(min(xdata)), np.log10(max(xdata)), 1000)\n",
    "        \n",
    "ax.plot(plot_x, power_func(plot_x, *popt),\n",
    "        color = 'DarkGreen',#'#92cac6',\n",
    "        linestyle = ':',\n",
    "        label = r'Model Fit: %1.2f$(\\frac{P}{[\\mu N s]})^{%1.2f}$'%(popt[0], popt[1]))\n",
    "\n",
    "ax.scatter(grouped['Ptot'], ydata, color = 'DarkGreen',#'#92cac6', \n",
    "           marker = 'x', s = 50, zorder = 10,\n",
    "           label = 'Model Prediction' )\n",
    "\n",
    "ax.set_title('Momentum Population Fit')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = impactList.plotMomSkyArea(credible = 0.99);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getXYEllipse(center, x_span, y_span, angle):\n",
    "    \n",
    "    x_points = np.linspace(-x_span, x_span, 30)\n",
    "    \n",
    "    #Get upper half of ellipse\n",
    "    #       -  \n",
    "    #   -       -\n",
    "    # -            -\n",
    "    y_list = []\n",
    "    for x in x_points:\n",
    "        y = np.sqrt((y_span ** 2) * (1 - (x / x_span) **2))\n",
    "        y_list.append(y)\n",
    "        \n",
    "        \n",
    "    # Create List\n",
    "    x_list = np.asarray(list(x_points) + list(x_points[::-1]))\n",
    "    minus_y = [-1 * y for y in y_list]\n",
    "    y_list = np.asarray(y_list + minus_y)\n",
    "        \n",
    "    #Rotate\n",
    "    x_new = [x_list[i] * np.cos(angle) - y_list[i] * np.sin(angle) \n",
    "             for i in range(len(x_list))]\n",
    "    y_new = [x_list[i] * np.sin(angle) + y_list[i] * np.cos(angle) \n",
    "             for i in range(len(x_list))]\n",
    "    \n",
    "    x_new += center[0]\n",
    "    y_new += center[1]\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "from  matplotlib.patches import Ellipse\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "import ligo.skymap.plot\n",
    "import healpy as hp\n",
    "\n",
    "\n",
    "def getEllipse(impact, percent_sky = .1, getPoints = False, rads = False):\n",
    "    if impact.skyArea_sun > percent_sky * 41253: #or impact.skyArea_sun == 0:\n",
    "        if impact.skyArea_sun == 0:\n",
    "            print(impact.gps, impact.N, len(impact.lon), impact.skyArea)\n",
    "        #print(impact.skyArea_sun, '>',  percent_sky * 41253)\n",
    "        return None, None\n",
    "        \n",
    "    if rads:\n",
    "        lons = np.deg2rad(impact.lon_sun) #getParam('lon')\n",
    "        lats = np.deg2rad(impact.lat_sun) #getParam('lat')\n",
    "    else:\n",
    "        lons = impact.lon_sun#getParam('lon')\n",
    "        lats = impact.lat_sun#getParam('lat')\n",
    "\n",
    "\n",
    "    x2 = max(lons)\n",
    "    x1 = min(lons)\n",
    "    y2 = max(lats)\n",
    "    y1 = min(lats)\n",
    "\n",
    "    y_length = (y2 - y1) / 2\n",
    "    x_length = (x2 - x1) / 2\n",
    "    \n",
    "    avg_x = sum(lons) / len(lons)\n",
    "    avg_y = sum(lats) / len(lats)\n",
    "    x_diff = [lon - avg_x for lon in lons]\n",
    "    y_diff = [lat - avg_y for lat in lats]\n",
    "\n",
    "    x_diff_squared = [element**2 for element in x_diff]\n",
    "    slope = sum(x * y for x,y in zip(x_diff, y_diff)) / sum(x_diff_squared)\n",
    "    angle = np.arctan(slope)\n",
    "    \n",
    "    if not rads:\n",
    "        angle = np.rad2deg(angle)\n",
    "    \n",
    "    if getPoints:\n",
    "        x, y = getXYEllipse([avg_x, avg_y], x_length / 2, y_length / 2, angle)\n",
    "        return x, y\n",
    "    else:\n",
    "        ells = (Ellipse((avg_x, avg_y), \n",
    "                 x_length, y_length, angle, \n",
    "                 faceColor = 'none', edgeColor = 'indianRed'))\n",
    "    return ells\n",
    "\n",
    "\n",
    "#fig1, ax1 = plt.subplots(figsize = (8,8))\n",
    "#ax1.set_ylim(-90, 90)\n",
    "#ax1.set_xlim(-180, 180)\n",
    "\n",
    "def plotEllipse(scatter = False):\n",
    "    fig, ax = plt.subplots(figsize = (10,10), \n",
    "                       subplot_kw={'projection': 'mollweide'})\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize = (10,10))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, 2 * len(impactList.impact_list)))\n",
    "    for i, impact in enumerate(impactList.impact_list):\n",
    "\n",
    "        x, y = getEllipse(impact, getPoints = True, rads = True, percent_sky = 0.05)\n",
    "        if x is None:\n",
    "            continue\n",
    "\n",
    "        ax.plot(x, y, color = colors[i + 1], label = impact.gps)\n",
    "        if scatter:\n",
    "            ax.scatter(np.deg2rad(impact.lon_sun), np.deg2rad(impact.lat_sun), \n",
    "                       color = colors[i + 1])\n",
    "        ax2.plot(np.rad2deg(x), np.rad2deg(y), \n",
    "                 color = colors[i + 1], label = impact.gps)\n",
    "        \n",
    "    return fig\n",
    "        \n",
    "plotEllipse(scatter = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getisGlitch(segment, df_veto):\n",
    "    try:\n",
    "        index = df_veto.index[df_veto['segment'] == int(segment)][0]\n",
    "    except IndexError:\n",
    "        # if not in the list, its not an Impact\n",
    "        return False\n",
    "    else:\n",
    "        return df_veto['isGlitch'].values[index]\n",
    "\n",
    "def getSearchTimes():\n",
    "    search_times = pd.read_csv(BASE_DIR + '/scripts/segment_list_NEW.txt',\n",
    "                               #dataDir + '/keyansegments.txt', \n",
    "                               #'/segment_list_NEW.txt',\n",
    "            header = 'infer', delim_whitespace = True)\n",
    "    \n",
    "    return search_times\n",
    "\n",
    "def getisValidSearch(segment, search_times):\n",
    "    # search_times = pd.read_csv(BASE_DIR + dataDir + '/segment_list_NEW.txt',\n",
    "    #        header = 'infer', delim_whitespace = True)\n",
    "    try:\n",
    "        index = search_times.index[search_times['segment'] == int(segment)][0]\n",
    "    except IndexError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def getisValidSearch_old(segment):\n",
    "    search_times = pd.read_csv(BASE_DIR + dataDir + '/segment_list.txt',\n",
    "            header = None, names = ['segment'], delim_whitespace = True)\n",
    "    try:\n",
    "        index = search_times.index[search_times['segment'] == int(segment)][0]\n",
    "    except IndexError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def isValid(segment, df_veto, search_times):\n",
    "    # Both checks if the time searched is valid as well as \n",
    "    # if the time is a glitch\n",
    "\n",
    "    if getisValidSearch(segment, search_times) and not getisGlitch(segment, df_veto):\n",
    "        return True\n",
    "\n",
    "    # If it's not in the segment list then its not a good segment\n",
    "    return False\n",
    "\n",
    "def get_start_stop(segments, seg_lens = None):\n",
    "    segments = np.asarray(segments)\n",
    "    sort = segments.argsort()\n",
    "    segments = segments[sort]\n",
    "    \n",
    "    if seg_lens is not None:\n",
    "        seg_lens = np.asarray(seg_lens)\n",
    "        seg_lens = seg_lens[sort]\n",
    "\n",
    "    start_stop = []\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        start = segments[i]\n",
    "        stop = 0\n",
    "    \n",
    "        count = 1\n",
    "        while (stop == 0 and (i + count < len(segments))):\n",
    "            if seg_lens is None:\n",
    "                if start + 1648 * count + 5 > segments[i + count]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    stop = segments[i + (count - 1)]\n",
    "            else:\n",
    "                if start + np.sum(seg_lens[i:(i + count)]) > segments[i + count]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    stop = segments[i + (count - 1)]\n",
    "                    \n",
    "        if (i + count >= len(segments)):\n",
    "            stop = segments[i + (count - 1)]\n",
    "    \n",
    "        if start == stop:\n",
    "            if seg_lens is None:\n",
    "                stop = start + 1648\n",
    "            else:\n",
    "                stop = start + seg_lens[i]\n",
    "        \n",
    "        start_stop.append([start, stop])\n",
    "    \n",
    "        i += count\n",
    "    return start_stop\n",
    "\n",
    "def timeline(self, dataDir  = dataDir, timeDir = BASE_DIR + '/data/observed_times',\n",
    "             filenames = ['DRSlistDec.txt', 'LTPCMNTlist.txt', 'LTPlist2.txt'],\n",
    "             segments = None):\n",
    "\n",
    "    #import datetime\n",
    "    #import time\n",
    "    from astropy.time import Time\n",
    "    \n",
    "    #segments = np.loadtxt(BASE_DIR + dataDir + '/' + segment_file)\n",
    "    \n",
    "    def formatTime(gps):\n",
    "        t0 = Time(gps, format='gps')\n",
    "        t0 = Time(t0, format='iso')\n",
    "        return t0.datetime\n",
    "    \n",
    "    print(\"Loading Valid Times\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (15, 3))\n",
    "    ax.set_title('Impact Timeline')\n",
    "    ax.yaxis.set_ticks_position('none') \n",
    "\n",
    "    regex = r'g_(\\d+)_(\\d+).tar.gz'\n",
    "    j = 0\n",
    "    colors = ['r', 'b', 'g']\n",
    "    \n",
    "    search_times = getSearchTimes()\n",
    "    \n",
    "    \n",
    "    LTP = search_times[search_times['control'] == 'LTP'].reset_index()\n",
    "    DRS = search_times[search_times['control'] == 'DRS'].reset_index()\n",
    "    BOTH = search_times[search_times['control'] == 'BOTH'].reset_index()\n",
    "    \n",
    "    controls = [LTP, DRS, BOTH]\n",
    "    \n",
    "    # Plot segments\n",
    "    \n",
    "    for c in controls:\n",
    "        print(type(c))\n",
    "        name = c['control'].values[0]\n",
    "        if name == 'LTP':\n",
    "            color = 'lightblue'\n",
    "        elif name == 'DRS':\n",
    "            color = 'pink'\n",
    "        else:\n",
    "            color = 'mediumslateblue'\n",
    "            \n",
    "        start_stop = get_start_stop(c['segment'], seg_lens = c['length'])\n",
    "        \n",
    "        for i, ss in enumerate(start_stop):\n",
    "            t0 = formatTime(ss[0])\n",
    "            t1 = formatTime(ss[1])\n",
    "                \n",
    "            if i == 0:\n",
    "                ax.axvspan(t0, t1, facecolor = color, alpha = 1, \n",
    "                           label = name)\n",
    "            else:\n",
    "                ax.axvspan(t0, t1, facecolor = color, alpha = 1)\n",
    "\n",
    "    #Plot impacts\n",
    "    for i, impact in enumerate(self.impact_list):\n",
    "        if i == 0: \n",
    "            ax.scatter(formatTime(impact.gps), .5, color = '#ffd27a', \n",
    "                       label = 'Impact', marker = 'o', s = 50, zorder = 10)\n",
    "        else:\n",
    "            ax.scatter(formatTime(impact.gps), .5, color = '#ffd27a', \n",
    "                       marker = 'o', s = 50, zorder = 10)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    # OLD \n",
    "    direc = \"/Users/shouriha/LISAPathfinder/scripts/Analysis/data/OLD/OLD_TIMES_0526\"\n",
    "    df_veto = impactList.impact_list[0].getVetoList()\n",
    "    search_times = getSearchTimes()\n",
    "    for root, dirs, files in os.walk(direc):\n",
    "        filenames = files\n",
    "    old_impact_inv = getGRSSegments(filenames)\n",
    "    \n",
    "    old_impact = []\n",
    "    for o in old_impact_inv:\n",
    "        if isValid(o, df_veto, search_times) and getisValidSearch_old(o):\n",
    "            #not getisGlitch(o, df_veto):\n",
    "            old_impact.append(o)\n",
    "        else:\n",
    "            print(o, 'not Valid')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    return fig\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timeline(impactList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Detections\n",
    "We are including some marginal detections in our dataset. \n",
    "We did not realize before because we were reading in the impactChain before the logL chain and making \n",
    "cuts based on that. This was not correct.\n",
    "We first must look at the likelihood chain, then take the second half of that as our trim. From here we see what our detection fraction is dfrac = N(impact = 1) / N(tot).\n",
    "\n",
    "We then read in N(impact = 1) from impactChain.dat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some of these chains are short? I am not sure what to do about it \n",
    "# These short chains are the marginal detections john found as maybe interesting\n",
    "# After looking at the telemetry they seem OK but the problem is that the chains are too short to really see \n",
    "# anything\n",
    "\n",
    "for impact1 in impactList.impact_list:\n",
    "    # Log likelihood chain to see mixing\n",
    "    \n",
    "    if len(impact1.lon) < 200000:\n",
    "        print('short', impact1.gps, impact1.run) \n",
    "        print('\\t dfrac', impact1.dfrac)\n",
    "    elif len(impact1.lon) > 500000:\n",
    "        print('long', impact1.gps)\n",
    "    else:\n",
    "        #print('good', impact1.gps)\n",
    "        #print('\\t', impact1.dfrac)\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    ax.set_ylabel(\"Log Likelihood\")\n",
    "    ax.plot(impact1.logL)\n",
    "    ax.set_title(impact1.segment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment List\n",
    "I am genuinely unsure how to handle the glitches that jake vetoed, but here is a list of all the segments together along with their command. This is mostly relevant for the bayesian analysis but it might still be relevant here. \n",
    "\n",
    "I tried to includes the times when both were running but it was more difficult than expected and I ran out of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = r'.*g_(\\d+)_(\\d+).tar.gz'\n",
    "timeDir = BASE_DIR + '/data/observed_times'\n",
    "filenames = ['DRSlistDec.txt', 'LTPCMNTlist.txt', 'LTPlist2.txt']\n",
    "write_file = open('segment_list_NEW.txt', 'w+')\n",
    "write_file.write('segment\\tlength\\tcontrol\\n')\n",
    "\n",
    "class seg:\n",
    "    def __init__(self, segment, length, command):\n",
    "        self.segment = segment\n",
    "        self.length = length\n",
    "        self.command = command\n",
    "        return\n",
    "\n",
    "segs = []\n",
    "for files in filenames:\n",
    "    t_file = timeDir +'/' + files\n",
    "    with open(t_file) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    if files == 'DRSlistDec.txt':\n",
    "        command = 'DRS'\n",
    "    elif files == 'LTPCMNTlist.txt':\n",
    "        command = 'BOTH'\n",
    "    else:\n",
    "        command = 'LTP'\n",
    "            \n",
    "    t0s = [int(re.findall(regex, c)[0][0]) for c in content]\n",
    "    seg_len = [int(re.findall(regex, c)[0][1]) for c in content]\n",
    "    \n",
    "    for i in range(len(t0s)):\n",
    "        segs.append(seg(t0s[i], seg_len[i], command))\n",
    "            \n",
    "    for i in range(len(t0s)):\n",
    "        write_file.write('%i\\t%i\\t%s\\n'%(t0s[i], seg_len[i], command))\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getObservedTime(segment, dt = 0):\n",
    "    #dt is time from start of segment\n",
    "    searchTimes = getSearchTimes()\n",
    "    searchTimes = searchTimes.sort_values('segment').reset_index()\n",
    "    searchTimes['total'] = searchTimes.length.cumsum()\n",
    "    \n",
    "    line = searchTimes.loc[searchTimes['segment'] == segment]\n",
    "    return line['total'].values[0]\n",
    "\n",
    "for impact0 in impactList.impact_list:\n",
    "    print(getObservedTime(impact0.segment, impact0.gps - impact0.segment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for impact0 in impactList.impact_list:\n",
    "    print(getObservedTime(impact0.segment, impact0.gps - impact0.segment))\n",
    "plt.plot(getObservedTime(impact0.segment, impact0.gps - impact0.segment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTex Table\n",
    "Creates a copy and pasteable table for LaTex documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\\begingroup\n",
      "\t\t\t\\renewcommand\\arraystretch{2}\n",
      "\t\t\t\\begin{longtable}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n",
      "\t\t\t\t\\multicolumn{9}{c}\n",
      "\t\t\t\t{{\\bfseries \\tablename\\  \\thetable{}}}\\\\\n",
      "\t\t\t\t\\hline \\multicolumn{1}{|c}{\\textbf{Date}} & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{GPS}}  & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\bf{$\\rho_{med}$ [$\\mu Ns$]}} & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{Face}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{Sky Area}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lat_{SC}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lon_{SC}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lat_{SSE}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lon_{SSE}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_X$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_Y$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_Z$}} \\\\\n",
      "\t\t\t\t\\hline\n",
      "\t\t\t\\endfirsthead\n",
      "\t\t\t\n",
      "\t\t\t\\multicolumn{9}{c}\n",
      "\t\t\t\t{{\\bfseries \\tablename\\  \\thetable{} -- continued from previous page}} \\\\\n",
      "\t\t\t\\hline \\multicolumn{1}{|c|}{\\textbf{Date}} & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{GPS}}  & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\bf{$\\rho_{med}$ [$\\mu Ns$]}} & \n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{Face}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{Sky Area}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lat_{SC}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lon_{SC}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lat_{SSE}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$Lon_{SSE}$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_X$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_Y$}} &\n",
      "\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{$LPF_Z$}} \\\\\n",
      "\t\t\t\t\\hline\n",
      "\t\t\t\\endhead\n",
      "\t\t\t\n",
      "\t\t\t\\hline \\multicolumn{9}{|r|}{{Continued on next page}} \\\\ \\hline\n",
      "\t\t\t\\endfoot\n",
      "\n",
      "\t\t\t\\hline\n",
      "\t\t\t\\endlastfoot\n",
      "\t2016-04-09 & 1144229908 & $17.2^{+0.4}_{-0.3}$ & +y+y & 1729 & -7 & -7 & -57 & -39 & 1.09 & 0.55 & -0.05 \\\\\n",
      "\t2016-05-04 & 1146429822 & $ 1.7^{+3.1}_{-0.6}$ & - & - & - & - & - & - & 0.45 & 1.24 & 0.56 \\\\\n",
      "\t2016-05-16 & 1147442122 & $ 0.7^{+0.5}_{-0.5}$ & - & - & - & - & - & - & 0.14 & 1.34 & 0.77 \\\\\n",
      "\t2016-05-16 & 1147453726 & $14.4^{+0.8}_{-0.4}$ & +x+x & 3438 & -2 & 162 & 45 & -56 & 0.14 & 1.34 & 0.77 \\\\\n",
      "\t2016-05-19 & 1147693044 & $ 0.9^{+0.9}_{-0.3}$ & - & - & - & - & - & - & 0.08 & 1.35 & 0.81 \\\\\n",
      "\t2016-05-19 & 1147741578 & $ 2.0^{+0.8}_{-0.3}$ & - & - & - & - & - & - & 0.06 & 1.35 & 0.82 \\\\\n",
      "\t2016-06-08 & 1149475988 & $ 1.0^{+1.1}_{-0.3}$ & - & - & - & - & - & - & -0.26 & 1.36 & 1.00 \\\\\n",
      "\t2016-06-20 & 1150511110 & $ 3.5^{+1.7}_{-1.2}$ & - & - & - & - & - & - & -0.35 & 1.33 & 1.03 \\\\\n",
      "\t2016-07-07 & 1151901050 & $ 0.2^{+0.5}_{-0.1}$ & - & - & - & - & - & - & -0.42 & 1.35 & 1.00 \\\\\n",
      "\t2016-07-24 & 1153404058 & $ 2.9^{+1.3}_{-0.3}$ & - & - & - & - & - & - & -0.48 & 1.37 & 0.87 \\\\\n",
      "\t2016-07-28 & 1153750663 & $19.9^{+1.7}_{-1.3}$ & +z+z & 2585 & 18 & 158 & -31 & -172 & -0.51 & 1.38 & 0.83 \\\\\n",
      "\t2016-07-31 & 1154024345 & $ 8.6^{+1.8}_{-1.6}$ & +x+y & 3857 & -7 & 128 & -7 & 156 & -0.54 & 1.38 & 0.79 \\\\\n",
      "\t2016-08-11 & 1154963503 & $ 2.4^{+0.8}_{-0.3}$ & - & - & - & - & - & - & -0.66 & 1.35 & 0.64 \\\\\n",
      "\t2016-08-17 & 1155461605 & $ 0.5^{+1.3}_{-0.3}$ & - & - & - & - & - & - & -0.73 & 1.31 & 0.54 \\\\\n",
      "\t2016-08-18 & 1155558407 & $ 1.6^{+1.3}_{-0.8}$ & - & - & - & - & - & - & -0.74 & 1.30 & 0.52 \\\\\n",
      "\t2016-08-19 & 1155637974 & $12.1^{+3.0}_{-3.2}$ & +z+z & 1786 & 68 & -87 & -4 & -39 & -0.76 & 1.29 & 0.50 \\\\\n",
      "\t2016-08-19 & 1155677822 & $ 2.4^{+1.7}_{-2.3}$ & +z+z & - & - & - & - & - & -0.76 & 1.29 & 0.50 \\\\\n",
      "\t2016-08-22 & 1155891413 & $ 0.7^{+0.3}_{-0.2}$ & - & - & - & - & - & - & -0.80 & 1.26 & 0.45 \\\\\n",
      "\t2016-08-23 & 1155985559 & $23.8^{+2.6}_{-2.1}$ & +z+z & 84 & 87 & -112 & -7 & -58 & -0.82 & 1.25 & 0.43 \\\\\n",
      "\t2016-08-23 & 1156020427 & $ 0.9^{+3.0}_{-0.7}$ & +z+z & - & - & - & - & - & -0.83 & 1.25 & 0.42 \\\\\n",
      "\t2016-08-24 & 1156063801 & $ 1.0^{+0.7}_{-0.8}$ & - & - & - & - & - & - & -0.83 & 1.24 & 0.41 \\\\\n",
      "\t2016-08-24 & 1156115516 & $ 3.0^{+1.0}_{-0.9}$ & +z+z & 1873 & 77 & -105 & -11 & -53 & -0.84 & 1.23 & 0.40 \\\\\n",
      "\t2016-08-25 & 1156188047 & $ 0.5^{+1.2}_{-0.4}$ & - & - & - & - & - & - & -0.86 & 1.22 & 0.39 \\\\\n",
      "\t2016-08-26 & 1156255314 & $ 0.6^{+2.8}_{-0.3}$ & - & - & - & - & - & - & -0.87 & 1.21 & 0.37 \\\\\n",
      "\t2016-09-15 & 1157966718 & $ 1.1^{+1.3}_{-0.4}$ & - & - & - & - & - & - & -1.14 & 0.70 & -0.04 \\\\\n",
      "\t2016-10-05 & 1159736213 & $ 0.9^{+1.5}_{-0.6}$ & +z+z & - & - & - & - & - & -1.15 & -0.18 & -0.41 \\\\\n",
      "\t2016-10-06 & 1159808666 & $230.3^{+4.8}_{-5.8}$ & +x+y & 430 & 4 & 101 & -62 & 116 & -1.14 & -0.21 & -0.42 \\\\\n",
      "\t2016-10-07 & 1159869088 & $ 6.4^{+2.8}_{-3.4}$ & +z+z & 2645 & 66 & 3 & -18 & 6 & -1.13 & -0.25 & -0.43 \\\\\n",
      "\t2016-12-02 & 1164719570 & $ 0.6^{+0.6}_{-0.3}$ & - & - & - & - & - & - & 0.06 & -1.62 & -0.29 \\\\\n",
      "\t2016-12-20 & 1166268578 & $ 8.0^{+3.1}_{-2.8}$ & - & - & - & - & - & - & 0.20 & -1.65 & -0.23 \\\\\n",
      "\t2016-12-21 & 1166337501 & $ 1.6^{+1.1}_{-0.4}$ & - & - & - & - & - & - & 0.21 & -1.65 & -0.23 \\\\\n",
      "\t2016-12-26 & 1166805122 & $ 0.5^{+1.1}_{-0.4}$ & - & - & - & - & - & - & 0.23 & -1.65 & -0.24 \\\\\n",
      "\t2016-12-27 & 1166921605 & $28.6^{+1.2}_{-0.9}$ & +y-x & 1716 & 19 & 13 & -12 & -91 & 0.24 & -1.65 & -0.24 \\\\\n",
      "\t2016-12-28 & 1166995369 & $ 0.8^{+0.9}_{-0.3}$ & - & - & - & - & - & - & 0.25 & -1.64 & -0.25 \\\\\n",
      "\t2017-01-01 & 1167307196 & $22.5^{+0.8}_{-0.7}$ & +x+x & 2149 & -7 & 150 & 17 & 114 & 0.26 & -1.64 & -0.26 \\\\\n",
      "\t2017-01-04 & 1167613479 & $ 0.9^{+1.0}_{-0.3}$ & - & - & - & - & - & - & 0.28 & -1.62 & -0.28 \\\\\n",
      "\t2017-01-05 & 1167654180 & $10.3^{+2.1}_{-1.5}$ & - & - & - & - & - & - & 0.28 & -1.62 & -0.28 \\\\\n",
      "\t2017-01-08 & 1167944728 & $ 4.5^{+0.6}_{-0.3}$ & -y-y & - & - & - & - & - & 0.30 & -1.61 & -0.30 \\\\\n",
      "\t2017-01-10 & 1168061759 & $ 3.5^{+0.9}_{-0.7}$ & -y-y & - & - & - & - & - & 0.30 & -1.60 & -0.31 \\\\\n",
      "\t2017-01-12 & 1168267680 & $ 1.2^{+1.0}_{-0.3}$ & - & - & - & - & - & - & 0.31 & -1.59 & -0.33 \\\\\n",
      "\t2017-02-12 & 1170979672 & $ 1.8^{+1.2}_{-0.4}$ & - & - & - & - & - & - & 0.68 & -1.22 & -0.60 \\\\\n",
      "\t2017-02-13 & 1171012017 & $ 2.5^{+2.2}_{-1.1}$ & - & - & - & - & - & - & 0.68 & -1.22 & -0.60 \\\\\n",
      "\t2017-03-11 & 1173291241 & $ 1.9^{+0.9}_{-0.3}$ & - & - & - & - & - & - & 1.12 & -0.38 & -0.55 \\\\\n",
      "\t2017-04-22 & 1176914535 & $ 1.0^{+1.0}_{-0.3}$ & - & - & - & - & - & - & 0.76 & 1.10 & 0.43 \\\\\n",
      "\t2017-04-22 & 1176917343 & $ 1.1^{+1.2}_{-0.3}$ & - & - & - & - & - & - & 0.76 & 1.10 & 0.43 \\\\\n",
      "\t2017-05-04 & 1177956916 & $ 1.3^{+1.1}_{-0.3}$ & - & - & - & - & - & - & 0.48 & 1.27 & 0.70 \\\\\n",
      "\t2017-05-05 & 1178035038 & $40.2^{+5.8}_{-6.6}$ & -y+x & 168 & -83 & -63 & -43 & -91 & 0.46 & 1.27 & 0.72 \\\\\n",
      "\t2017-05-06 & 1178120384 & $ 1.5^{+1.0}_{-0.3}$ & - & - & - & - & - & - & 0.44 & 1.28 & 0.73 \\\\\n",
      "\t2017-05-07 & 1178197245 & $ 1.2^{+1.1}_{-0.3}$ & - & - & - & - & - & - & 0.43 & 1.29 & 0.75 \\\\\n",
      "\t2017-05-08 & 1178251226 & $11.7^{+0.9}_{-0.3}$ & -y-y & - & - & - & - & - & 0.41 & 1.29 & 0.76 \\\\\n",
      "\t2017-05-18 & 1179167273 & $14.0^{+3.9}_{-2.5}$ & +x+y & 3015 & 8 & 84 & 27 & -142 & 0.23 & 1.33 & 0.93 \\\\\n",
      "\t2017-05-22 & 1179493289 & $ 8.0^{+0.8}_{-0.5}$ & +y-x & 3864 & -1 & 25 & -27 & -173 & 0.18 & 1.34 & 0.97 \\\\\n",
      "\t2017-06-04 & 1180613326 & $ 1.2^{+1.5}_{-0.4}$ & - & - & - & - & - & - & 0.02 & 1.33 & 1.08 \\\\\n",
      "\t2017-06-11 & 1181272382 & $ 1.0^{+1.0}_{-0.3}$ & - & - & - & - & - & - & -0.02 & 1.33 & 1.11 \\\\\n",
      "\t\\hline\n",
      "\\end{longtable} \n",
      "\\endgroup\n"
     ]
    }
   ],
   "source": [
    "# Make Summary table\n",
    "# Make sure \\usepackage{longtable} is at the beginning of the doc\n",
    "lateX_file = open(BASE_DIR + dataDir + '/lateX_table.tec', 'w+')\n",
    "summary_table = impactList.summaryTable(percent_sky = 0.1,ephem=dat)\n",
    "lateX_file.write(summary_table)\n",
    "lateX_file.close()\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just get one line from the summary table if we'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impactList.impact_list[0].summaryString()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
